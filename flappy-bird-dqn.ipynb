{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# --- Kaggle / Headless environment fixes (inserted) ---\nimport os\n# Use dummy SDL drivers so pygame works in headless Kaggle environment\nos.environ.setdefault(\"SDL_VIDEODRIVER\", \"dummy\")\nos.environ.setdefault(\"SDL_AUDIODRIVER\", \"dummy\")\n# Put checkpoints in /kaggle/working for persistence on Kaggle\nDEFAULT_CHECKPOINT_DIR = \"/kaggle/working\"\nos.makedirs(DEFAULT_CHECKPOINT_DIR, exist_ok=True)\n# Torch device selection (will use GPU on Kaggle if available)\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# Helper to resolve checkpoint paths\ndef resolve_checkpoint_path(filename=\"checkpoint.pth\"):\n    return os.path.join(DEFAULT_CHECKPOINT_DIR, filename)\n# End header\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T18:49:47.551898Z","iopub.execute_input":"2025-10-03T18:49:47.552400Z","iopub.status.idle":"2025-10-03T18:49:47.558210Z","shell.execute_reply.started":"2025-10-03T18:49:47.552377Z","shell.execute_reply":"2025-10-03T18:49:47.557381Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"\n# Safe pygame display initialization (headless-friendly)\ndef safe_set_mode(*args, **kwargs):\n    import pygame\n    try:\n        return safe_set_mode(*args, **kwargs)\n    except Exception as e:\n        try:\n            # Try initializing display explicitly and retry\n            pygame.display.init()\n            return safe_set_mode(*args, **kwargs)\n        except Exception as e2:\n            print(\"Warning: pygame display not available (headless). Using dummy surface. Errors:\", e, e2)\n            # Return a dummy Surface of requested size if possible\n            if len(args) >= 1 and isinstance(args[0], (tuple, list)):\n                w,h = args[0][0], args[0][1]\n                return pygame.Surface((w,h))\n            return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T18:49:47.559184Z","iopub.execute_input":"2025-10-03T18:49:47.559379Z","iopub.status.idle":"2025-10-03T18:49:47.571554Z","shell.execute_reply.started":"2025-10-03T18:49:47.559356Z","shell.execute_reply":"2025-10-03T18:49:47.570974Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom collections import deque\nimport math","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.572092Z","iopub.execute_input":"2025-10-03T18:49:47.572271Z","iopub.status.idle":"2025-10-03T18:49:47.582584Z","shell.execute_reply.started":"2025-10-03T18:49:47.572258Z","shell.execute_reply":"2025-10-03T18:49:47.581870Z"},"trusted":true},"outputs":[],"execution_count":90},{"cell_type":"code","source":"import os\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-03T18:49:47.584036Z","iopub.execute_input":"2025-10-03T18:49:47.584243Z","iopub.status.idle":"2025-10-03T18:49:47.591848Z","shell.execute_reply.started":"2025-10-03T18:49:47.584229Z","shell.execute_reply":"2025-10-03T18:49:47.591306Z"},"trusted":true},"outputs":[],"execution_count":91},{"cell_type":"code","source":"EPI_NUMS = 4000\nCHECKPOINT_PATH = resolve_checkpoint_path(resolve_checkpoint_path(\"checkpoint.pth\"))\nBUFFER_SIZE = 50000\nBATCH_SIZE = 64\nGAMMA = 0.99\nLR = 1e-4\nTAU = 0.02             \nEPSILON_DECAY = 20000\nEPSILON_START = 1.0\nEPSILON_FINAL = 0.05\nWARMUP_STEPS = 5000\nMAX_STEPS = 2000","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.592591Z","iopub.execute_input":"2025-10-03T18:49:47.593296Z","iopub.status.idle":"2025-10-03T18:49:47.601725Z","shell.execute_reply.started":"2025-10-03T18:49:47.593274Z","shell.execute_reply":"2025-10-03T18:49:47.601176Z"},"trusted":true},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = map(np.array, zip(*batch))\n        return (\n            torch.FloatTensor(state),\n            torch.LongTensor(action),\n            torch.FloatTensor(reward),\n            torch.FloatTensor(next_state),\n            torch.FloatTensor(done),\n        )\n\n    def __len__(self):\n        return len(self.buffer)","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.602500Z","iopub.execute_input":"2025-10-03T18:49:47.602868Z","iopub.status.idle":"2025-10-03T18:49:47.612750Z","shell.execute_reply.started":"2025-10-03T18:49:47.602852Z","shell.execute_reply":"2025-10-03T18:49:47.612178Z"},"trusted":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.613373Z","iopub.execute_input":"2025-10-03T18:49:47.613577Z","iopub.status.idle":"2025-10-03T18:49:47.625334Z","shell.execute_reply.started":"2025-10-03T18:49:47.613563Z","shell.execute_reply":"2025-10-03T18:49:47.624821Z"},"trusted":true},"outputs":[],"execution_count":94},{"cell_type":"code","source":"class Agent:\n    def __init__(self, state_dim=4, n_actions=2):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.n_actions = n_actions\n        self.policy_net = DQN(state_dim, n_actions).to(self.device)\n        self.target_net = DQN(state_dim, n_actions).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n        self.loss_fn = nn.SmoothL1Loss()\n\n    def act(self, state, epsilon):\n        if random.random() < epsilon:\n            return random.randint(0, self.n_actions - 1)\n        state_v = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        q_vals = self.policy_net(state_v)\n        return q_vals.argmax().item()\n\n    def update(self, buffer, batch_size):\n        if len(buffer) < batch_size:\n            return None\n        state, action, reward, next_state, done = buffer.sample(batch_size)\n        state, action, reward, next_state, done = (\n            state.to(self.device),\n            action.to(self.device),\n            reward.to(self.device),\n            next_state.to(self.device),\n            done.to(self.device),\n        )\n\n        q_values = self.policy_net(state).gather(1, action.unsqueeze(1)).squeeze(1)\n        next_q_values = self.target_net(next_state).max(1)[0]\n        expected_q = reward + GAMMA * next_q_values * (1 - done)\n\n        loss = self.loss_fn(q_values, expected_q.detach())\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # Soft target update (Polyak averaging)\n        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n            target_param.data.copy_(TAU * param.data + (1.0 - TAU) * target_param.data)\n\n        return loss.item()\n\n    def save(self, path):\n        \"\"\"Save policy, target networks and optimizer state\"\"\"\n        checkpoint = {\n            'policy_state_dict': self.policy_net.state_dict(),\n            'target_state_dict': self.target_net.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }\n        torch.save(checkpoint, path)\n    \n    def load(self, path):\n        \"\"\"Load checkpoint with safety checks\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n    \n        if 'policy_state_dict' in checkpoint:\n            self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n        else:\n            raise KeyError(\"Checkpoint missing 'policy_state_dict'\")\n    \n        if 'target_state_dict' in checkpoint:\n            self.target_net.load_state_dict(checkpoint['target_state_dict'])\n        else:\n            # fallback: copy policy_net\n            self.target_net.load_state_dict(self.policy_net.state_dict())\n    \n        if 'optimizer_state_dict' in checkpoint:\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        else:\n            print(\"Warning: optimizer state not found in checkpoint, skipping.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.625904Z","iopub.execute_input":"2025-10-03T18:49:47.626098Z","iopub.status.idle":"2025-10-03T18:49:47.636530Z","shell.execute_reply.started":"2025-10-03T18:49:47.626084Z","shell.execute_reply":"2025-10-03T18:49:47.635786Z"},"trusted":true},"outputs":[],"execution_count":95},{"cell_type":"code","source":"\nclass FlappyBirdEnv:\n    \"\"\"Compact, headless environment optimized for training.\n    Observation: [dy_norm, vel_norm, pipe_dist_norm, gap_y_norm]\n    Action: 0 = noop, 1 = flap\n    \"\"\"\n    def __init__(self, difficulty='normal', seed=None):\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n\n        presets = {\n            'easy':  {'PIPE_GAP':220,'PIPE_SPACING':300,'SCROLL_SPEED':2},\n            'normal':{'PIPE_GAP':180,'PIPE_SPACING':280,'SCROLL_SPEED':3},\n            'hard':  {'PIPE_GAP':150,'PIPE_SPACING':260,'SCROLL_SPEED':3}\n        }\n        if difficulty not in presets:\n            raise ValueError('difficulty must be one of: %s' % list(presets.keys()))\n\n        p = presets[difficulty]\n        self.PIPE_GAP = p['PIPE_GAP']\n        self.PIPE_SPACING = p['PIPE_SPACING']\n        self.SCROLL_SPEED = p['SCROLL_SPEED']\n\n        # Physics\n        self.GRAVITY = 0.5\n        self.FLAP_VEL = -9.0\n        self.MAX_VEL = 12.0\n\n        # Simple constants (no assets)\n        self.SCREEN_WIDTH = 288\n        self.SCREEN_HEIGHT = 512\n        self.GROUND_HEIGHT = 112\n        self.pipe_width = 52\n        self.INIT_PIPE_OFFSET = 100\n        self.MIN_GAP_Y = 50\n\n        # Reward params\n        self.LIVING_REWARD = 0.1\n        self.SCORE_REWARD = 10.0\n        self.DEATH_PENALTY = -7\n        self.VERTICAL_WEIGHT = 0.3\n        self.VELOCITY_WEIGHT = 0.1\n        self.CENTER_BONUS_MULT = 5.0\n        self.APPROACHING_THRESHOLD = 0.3\n        self.APPROACHING_MULTIPLIER = 2.0\n\n        self.reset()\n\n    def reset(self):\n        self.bird_x = 80\n        self.bird_y = self.SCREEN_HEIGHT//2\n        self.bird_vel = 0.0\n        self.bird_radius = 12\n\n        self.bg_x = 0\n        self.base_x = 0\n\n        self.pipes = []\n        start_x = self.SCREEN_WIDTH + self.INIT_PIPE_OFFSET\n        for i in range(3):\n            gap_y = random.randint(self.MIN_GAP_Y, self.SCREEN_HEIGHT - self.MIN_GAP_Y - self.PIPE_GAP)\n            self.pipes.append([start_x + i*self.PIPE_SPACING, gap_y])\n\n        self.done = False\n        self.score = 0\n        self.scored_pipes = set()\n        return self._get_state()\n\n    def step(self, action):\n        if action == 1:\n            self.bird_vel = self.FLAP_VEL\n\n        self.bird_vel += self.GRAVITY\n        self.bird_vel = max(-self.MAX_VEL, min(self.MAX_VEL, self.bird_vel))\n        self.bird_y += self.bird_vel\n\n        # Move pipes\n        for p in self.pipes:\n            p[0] -= self.SCROLL_SPEED\n\n        if len(self.pipes)>0 and (self.pipes[0][0] + self.pipe_width) < 0:\n            self.pipes.pop(0)\n            new_x = self.pipes[-1][0] + self.PIPE_SPACING\n            new_gap_y = random.randint(self.MIN_GAP_Y, self.SCREEN_HEIGHT - self.MIN_GAP_Y - self.PIPE_GAP)\n            self.pipes.append([new_x, new_gap_y])\n\n        if self._check_collision():\n            self.done = True\n            return self._get_state(), self.DEATH_PENALTY, True, {}\n\n        just_scored = False\n        for pipe in self.pipes:\n            px, gy = pipe\n            if (px + self.pipe_width) < self.bird_x and id(pipe) not in self.scored_pipes:\n                if gy < self.bird_y < gy + self.PIPE_GAP:\n                    self.score += 1\n                    just_scored = True\n                self.scored_pipes.add(id(pipe))\n\n        dy_norm, vel_norm, dx_norm = self._get_normalized_values()\n        if just_scored:\n            reward = self.SCORE_REWARD + max(0, (1.0 - abs(dy_norm)) * self.CENTER_BONUS_MULT)\n        else:\n            reward = self.LIVING_REWARD - abs(dy_norm)*self.VERTICAL_WEIGHT - abs(vel_norm)*self.VELOCITY_WEIGHT\n            if dx_norm < self.APPROACHING_THRESHOLD:\n                reward *= self.APPROACHING_MULTIPLIER\n\n        return self._get_state(), reward, self.done, {}\n\n    def _check_collision(self):\n        GROUND_Y = self.SCREEN_HEIGHT - self.GROUND_HEIGHT\n        if self.bird_y - self.bird_radius <= 0 or self.bird_y + self.bird_radius >= GROUND_Y:\n            return True\n        for px, gy in self.pipes:\n            if (self.bird_x + self.bird_radius) > px and (self.bird_x - self.bird_radius) < (px + self.pipe_width):\n                if self.bird_y - self.bird_radius < gy or self.bird_y + self.bird_radius > gy + self.PIPE_GAP:\n                    return True\n        return False\n\n    def _get_next_pipe(self):\n        for px, gy in self.pipes:\n            if px + self.pipe_width >= self.bird_x:\n                return px, gy\n        return self.pipes[0]\n\n    def _get_normalized_values(self):\n        next_px, next_gy = self._get_next_pipe()\n        gap_center = next_gy + self.PIPE_GAP/2\n        dy_norm = (gap_center - self.bird_y)/float(self.PIPE_GAP)\n        vel_norm = self.bird_vel/float(self.MAX_VEL)\n        dx_norm = (next_px - self.bird_x)/float(self.SCREEN_WIDTH)\n        return dy_norm, vel_norm, dx_norm\n\n    def _get_state(self):\n        next_px, next_gy = self._get_next_pipe()\n        gap_center = next_gy + self.PIPE_GAP/2\n        dy_norm = (gap_center - self.bird_y)/float(self.PIPE_GAP)\n        vel_norm = self.bird_vel/float(self.MAX_VEL)\n        pipe_dist_norm = (next_px - self.bird_x)/float(self.SCREEN_WIDTH)\n        gap_y_norm = next_gy/float(self.SCREEN_HEIGHT)\n        return np.array([dy_norm, vel_norm, pipe_dist_norm, gap_y_norm], dtype=np.float32)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.739177Z","iopub.execute_input":"2025-10-03T18:49:47.739366Z","iopub.status.idle":"2025-10-03T18:49:47.754604Z","shell.execute_reply.started":"2025-10-03T18:49:47.739350Z","shell.execute_reply":"2025-10-03T18:49:47.753902Z"},"trusted":true},"outputs":[],"execution_count":96},{"cell_type":"code","source":"def train_loop(num_episodes=EPI_NUMS, resume=False, difficulty=\"normal\"):\n    env = FlappyBirdEnv(difficulty=difficulty)\n    agent = Agent()\n    buffer = ReplayBuffer(BUFFER_SIZE)\n\n    if resume and os.path.exists(CHECKPOINT_PATH):\n        print(\"Loading checkpoint...\")\n        agent.load(CHECKPOINT_PATH)\n        print(\"Loaded model.\")\n\n    total_steps = 0\n    losses, all_scores = [], []\n\n    # Warmup\n    print(f\"Collecting {WARMUP_STEPS} random transitions for warmup...\")\n    state = env.reset()\n    for _ in range(WARMUP_STEPS):\n        action = random.randint(0, 1)\n        next_state, reward, done, _ = env.step(action)\n        buffer.push(state, action, reward, next_state, float(done))\n        state = env.reset() if done else next_state\n    print(f\"Warmup finished. Replay buffer size = {len(buffer)}\")\n\n    for ep in range(1, num_episodes + 1):\n        state = env.reset()\n        ep_reward, done = 0.0, False\n\n        while not done:\n            epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * max(0, (1 - total_steps / EPSILON_DECAY))\n            action = agent.act(state, epsilon)\n            next_state, reward, done, _ = env.step(action)\n            buffer.push(state, action, reward, next_state, float(done))\n\n            loss = agent.update(buffer, batch_size=BATCH_SIZE)\n            if loss is not None:\n                losses.append(loss)\n\n            state = next_state\n            ep_reward += reward\n            total_steps += 1\n\n        all_scores.append(env.score)\n        if ep % 50 == 0:\n            avg_score = np.mean(all_scores[-50:])\n            avg_loss = np.mean(losses[-100:]) if len(losses) > 0 else 0.0\n            print(f\"Ep {ep:4d} | Steps {total_steps:6d} | Score {env.score:3d} | \"\n                  f\"EpReward {ep_reward:.2f} | Eps {epsilon:.3f} | \"\n                  f\"AvgScore50 {avg_score:.2f} | AvgLoss100 {avg_loss:.4f}\")\n            agent.save(CHECKPOINT_PATH)\n\n    agent.save(CHECKPOINT_PATH)\n    print(\"✅ Training finished. Model saved to\", CHECKPOINT_PATH)","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.755848Z","iopub.execute_input":"2025-10-03T18:49:47.756378Z","iopub.status.idle":"2025-10-03T18:49:47.769862Z","shell.execute_reply.started":"2025-10-03T18:49:47.756357Z","shell.execute_reply":"2025-10-03T18:49:47.769169Z"},"trusted":true},"outputs":[],"execution_count":97},{"cell_type":"code","source":"train_loop(num_episodes=EPI_NUMS, difficulty=\"normal\", resume=False)","metadata":{"execution":{"iopub.status.busy":"2025-10-03T18:49:47.770464Z","iopub.execute_input":"2025-10-03T18:49:47.770647Z","iopub.status.idle":"2025-10-03T19:12:05.566546Z","shell.execute_reply.started":"2025-10-03T18:49:47.770634Z","shell.execute_reply":"2025-10-03T19:12:05.565772Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting 5000 random transitions for warmup...\nWarmup finished. Replay buffer size = 5000\nEp   50 | Steps   1626 | Score   0 | EpReward -10.04 | Eps 0.923 | AvgScore50 0.00 | AvgLoss100 0.0742\nEp  100 | Steps   3274 | Score   0 | EpReward -14.99 | Eps 0.845 | AvgScore50 0.00 | AvgLoss100 0.2067\nEp  150 | Steps   4938 | Score   0 | EpReward -10.35 | Eps 0.765 | AvgScore50 0.00 | AvgLoss100 0.1835\nEp  200 | Steps   6629 | Score   0 | EpReward -13.35 | Eps 0.685 | AvgScore50 0.00 | AvgLoss100 0.1104\nEp  250 | Steps   8396 | Score   0 | EpReward -9.26 | Eps 0.601 | AvgScore50 0.00 | AvgLoss100 0.0715\nEp  300 | Steps  10264 | Score   0 | EpReward -9.22 | Eps 0.513 | AvgScore50 0.00 | AvgLoss100 0.0550\nEp  350 | Steps  12108 | Score   0 | EpReward -9.03 | Eps 0.425 | AvgScore50 0.00 | AvgLoss100 0.0566\nEp  400 | Steps  14209 | Score   0 | EpReward -13.54 | Eps 0.325 | AvgScore50 0.00 | AvgLoss100 0.0360\nEp  450 | Steps  16723 | Score   0 | EpReward -14.46 | Eps 0.206 | AvgScore50 0.00 | AvgLoss100 0.0501\nEp  500 | Steps  20707 | Score   1 | EpReward -7.96 | Eps 0.050 | AvgScore50 0.02 | AvgLoss100 0.1872\nEp  550 | Steps  25257 | Score   0 | EpReward -8.43 | Eps 0.050 | AvgScore50 0.16 | AvgLoss100 0.2014\nEp  600 | Steps  31599 | Score   1 | EpReward -4.79 | Eps 0.050 | AvgScore50 0.48 | AvgLoss100 0.0991\nEp  650 | Steps  37870 | Score   0 | EpReward -4.72 | Eps 0.050 | AvgScore50 0.46 | AvgLoss100 0.0503\nEp  700 | Steps  44871 | Score   1 | EpReward -2.40 | Eps 0.050 | AvgScore50 0.56 | AvgLoss100 0.0787\nEp  750 | Steps  51840 | Score   0 | EpReward -17.56 | Eps 0.050 | AvgScore50 0.56 | AvgLoss100 0.1050\nEp  800 | Steps  60400 | Score   2 | EpReward -4.77 | Eps 0.050 | AvgScore50 0.90 | AvgLoss100 0.1404\nEp  850 | Steps  67383 | Score   0 | EpReward -8.06 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.1272\nEp  900 | Steps  74034 | Score   0 | EpReward -6.35 | Eps 0.050 | AvgScore50 0.52 | AvgLoss100 0.1125\nEp  950 | Steps  81394 | Score   0 | EpReward -10.01 | Eps 0.050 | AvgScore50 0.72 | AvgLoss100 0.1118\nEp 1000 | Steps  87657 | Score   0 | EpReward -11.92 | Eps 0.050 | AvgScore50 0.42 | AvgLoss100 0.1267\nEp 1050 | Steps  93272 | Score   0 | EpReward -13.25 | Eps 0.050 | AvgScore50 0.38 | AvgLoss100 0.1374\nEp 1100 | Steps  98668 | Score   0 | EpReward -10.24 | Eps 0.050 | AvgScore50 0.28 | AvgLoss100 0.1415\nEp 1150 | Steps 104597 | Score   0 | EpReward -11.64 | Eps 0.050 | AvgScore50 0.38 | AvgLoss100 0.1123\nEp 1200 | Steps 112919 | Score   4 | EpReward 31.78 | Eps 0.050 | AvgScore50 0.86 | AvgLoss100 0.1224\nEp 1250 | Steps 119216 | Score   1 | EpReward 1.90 | Eps 0.050 | AvgScore50 0.46 | AvgLoss100 0.1423\nEp 1300 | Steps 125116 | Score   0 | EpReward -21.67 | Eps 0.050 | AvgScore50 0.38 | AvgLoss100 0.1081\nEp 1350 | Steps 132215 | Score   1 | EpReward 4.27 | Eps 0.050 | AvgScore50 0.60 | AvgLoss100 0.1176\nEp 1400 | Steps 139410 | Score   1 | EpReward -12.75 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.1296\nEp 1450 | Steps 147225 | Score   6 | EpReward 51.51 | Eps 0.050 | AvgScore50 0.78 | AvgLoss100 0.1192\nEp 1500 | Steps 153352 | Score   1 | EpReward 6.61 | Eps 0.050 | AvgScore50 0.36 | AvgLoss100 0.1286\nEp 1550 | Steps 159846 | Score   0 | EpReward -4.18 | Eps 0.050 | AvgScore50 0.40 | AvgLoss100 0.0989\nEp 1600 | Steps 166463 | Score   0 | EpReward -13.01 | Eps 0.050 | AvgScore50 0.42 | AvgLoss100 0.0891\nEp 1650 | Steps 173601 | Score   1 | EpReward 11.00 | Eps 0.050 | AvgScore50 0.50 | AvgLoss100 0.0898\nEp 1700 | Steps 180353 | Score   3 | EpReward 7.95 | Eps 0.050 | AvgScore50 0.48 | AvgLoss100 0.0910\nEp 1750 | Steps 188004 | Score   0 | EpReward -11.41 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.1445\nEp 1800 | Steps 194763 | Score   0 | EpReward -4.40 | Eps 0.050 | AvgScore50 0.44 | AvgLoss100 0.0886\nEp 1850 | Steps 201959 | Score   0 | EpReward -21.45 | Eps 0.050 | AvgScore50 0.52 | AvgLoss100 0.1051\nEp 1900 | Steps 209216 | Score   2 | EpReward 19.73 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.1094\nEp 1950 | Steps 216741 | Score   1 | EpReward -1.77 | Eps 0.050 | AvgScore50 0.64 | AvgLoss100 0.0761\nEp 2000 | Steps 223906 | Score   4 | EpReward 37.25 | Eps 0.050 | AvgScore50 0.54 | AvgLoss100 0.1132\nEp 2050 | Steps 229725 | Score   1 | EpReward -1.01 | Eps 0.050 | AvgScore50 0.36 | AvgLoss100 0.0981\nEp 2100 | Steps 238140 | Score   0 | EpReward -8.06 | Eps 0.050 | AvgScore50 0.82 | AvgLoss100 0.0981\nEp 2150 | Steps 245055 | Score   0 | EpReward -5.92 | Eps 0.050 | AvgScore50 0.52 | AvgLoss100 0.0919\nEp 2200 | Steps 253037 | Score   0 | EpReward -6.74 | Eps 0.050 | AvgScore50 0.76 | AvgLoss100 0.0713\nEp 2250 | Steps 260661 | Score   0 | EpReward -24.84 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.0718\nEp 2300 | Steps 267370 | Score   1 | EpReward 5.35 | Eps 0.050 | AvgScore50 0.50 | AvgLoss100 0.1154\nEp 2350 | Steps 274789 | Score   0 | EpReward -8.21 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.0906\nEp 2400 | Steps 282463 | Score   0 | EpReward -11.86 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.1028\nEp 2450 | Steps 290117 | Score   0 | EpReward -20.69 | Eps 0.050 | AvgScore50 0.64 | AvgLoss100 0.1191\nEp 2500 | Steps 297067 | Score   1 | EpReward 6.62 | Eps 0.050 | AvgScore50 0.44 | AvgLoss100 0.1183\nEp 2550 | Steps 304300 | Score   0 | EpReward -13.26 | Eps 0.050 | AvgScore50 0.56 | AvgLoss100 0.0674\nEp 2600 | Steps 311689 | Score   1 | EpReward -8.73 | Eps 0.050 | AvgScore50 0.64 | AvgLoss100 0.0926\nEp 2650 | Steps 319339 | Score   2 | EpReward 17.88 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.0825\nEp 2700 | Steps 326608 | Score   2 | EpReward -0.02 | Eps 0.050 | AvgScore50 0.54 | AvgLoss100 0.0680\nEp 2750 | Steps 334268 | Score   2 | EpReward 12.91 | Eps 0.050 | AvgScore50 0.70 | AvgLoss100 0.0897\nEp 2800 | Steps 341369 | Score   0 | EpReward -11.84 | Eps 0.050 | AvgScore50 0.60 | AvgLoss100 0.1005\nEp 2850 | Steps 348175 | Score   2 | EpReward 23.12 | Eps 0.050 | AvgScore50 0.48 | AvgLoss100 0.0913\nEp 2900 | Steps 355613 | Score   1 | EpReward 1.32 | Eps 0.050 | AvgScore50 0.68 | AvgLoss100 0.0958\nEp 2950 | Steps 361968 | Score   0 | EpReward -17.77 | Eps 0.050 | AvgScore50 0.42 | AvgLoss100 0.1225\nEp 3000 | Steps 368604 | Score   2 | EpReward 16.31 | Eps 0.050 | AvgScore50 0.42 | AvgLoss100 0.1014\nEp 3050 | Steps 375767 | Score   2 | EpReward 23.53 | Eps 0.050 | AvgScore50 0.60 | AvgLoss100 0.0755\nEp 3100 | Steps 382209 | Score   0 | EpReward -9.32 | Eps 0.050 | AvgScore50 0.44 | AvgLoss100 0.0955\nEp 3150 | Steps 389879 | Score   1 | EpReward -13.66 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.0924\nEp 3200 | Steps 397778 | Score   0 | EpReward -11.17 | Eps 0.050 | AvgScore50 0.70 | AvgLoss100 0.0677\nEp 3250 | Steps 404028 | Score   0 | EpReward -6.12 | Eps 0.050 | AvgScore50 0.42 | AvgLoss100 0.0989\nEp 3300 | Steps 409707 | Score   1 | EpReward 2.70 | Eps 0.050 | AvgScore50 0.20 | AvgLoss100 0.0674\nEp 3350 | Steps 416688 | Score   1 | EpReward -12.85 | Eps 0.050 | AvgScore50 0.52 | AvgLoss100 0.1054\nEp 3400 | Steps 423364 | Score   0 | EpReward -7.86 | Eps 0.050 | AvgScore50 0.46 | AvgLoss100 0.0845\nEp 3450 | Steps 430891 | Score   0 | EpReward -20.62 | Eps 0.050 | AvgScore50 0.62 | AvgLoss100 0.0770\nEp 3500 | Steps 437409 | Score   2 | EpReward 4.75 | Eps 0.050 | AvgScore50 0.44 | AvgLoss100 0.0845\nEp 3550 | Steps 443545 | Score   2 | EpReward 8.75 | Eps 0.050 | AvgScore50 0.30 | AvgLoss100 0.0970\nEp 3600 | Steps 450060 | Score   0 | EpReward -24.85 | Eps 0.050 | AvgScore50 0.38 | AvgLoss100 0.0885\nEp 3650 | Steps 457011 | Score   1 | EpReward 8.30 | Eps 0.050 | AvgScore50 0.50 | AvgLoss100 0.1048\nEp 3700 | Steps 463114 | Score   0 | EpReward -10.61 | Eps 0.050 | AvgScore50 0.30 | AvgLoss100 0.0893\nEp 3750 | Steps 470689 | Score   1 | EpReward 5.68 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.0853\nEp 3800 | Steps 478182 | Score   0 | EpReward -14.99 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.1182\nEp 3850 | Steps 485409 | Score   1 | EpReward 3.87 | Eps 0.050 | AvgScore50 0.56 | AvgLoss100 0.1242\nEp 3900 | Steps 492138 | Score   0 | EpReward -7.84 | Eps 0.050 | AvgScore50 0.44 | AvgLoss100 0.0691\nEp 3950 | Steps 499512 | Score   1 | EpReward -0.23 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.0965\nEp 4000 | Steps 506785 | Score   1 | EpReward -26.07 | Eps 0.050 | AvgScore50 0.58 | AvgLoss100 0.0766\n✅ Training finished. Model saved to /kaggle/working/checkpoint.pth\n","output_type":"stream"}],"execution_count":98}]}